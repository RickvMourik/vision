{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "colab_type": "code",
    "id": "owuMk_guREX5",
    "outputId": "ba7cd846-f092-473b-e412-32461a6d779b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "from keras.utils import to_categorical\n",
    "from keras.datasets import cifar10\n",
    "import keras\n",
    "from scripts.load_data import load_train, load_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN bouwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p_lLaBNewZk0"
   },
   "outputs": [],
   "source": [
    "# Het importeren en bewerken van de data \n",
    "train_images, train_labels = load_train()\n",
    "test_images, test_labels = load_test()\n",
    "\n",
    "# Normalizeren van de images\n",
    "train_images = (train_images / 255) - 0.5\n",
    "test_images = (test_images / 255) - 0.5\n",
    "\n",
    "# Reshapen van de images zodat ze de juiste dimensies hebben\n",
    "train_images = np.expand_dims(train_images, axis=3)\n",
    "test_images = np.expand_dims(test_images, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nFleZ8yEyFtk"
   },
   "outputs": [],
   "source": [
    "# Onze CNN\n",
    "# Stap 1: bepaal hoeveel filters je wilt, hoe groot je filter size moet zijn (let op je filter size mag niet te groot zijn vergeleken met je images), en wat je pool size is. \n",
    "num_filters = 10\n",
    "filter_size = 3\n",
    "pool_size = 2\n",
    "\n",
    "# Stap 2: maak het model.\n",
    "#    In de array die je aan sequential meegeeft, zet je alle layers die in het model moeten:\n",
    "#    Conv2D, parameters: num_filters, filter_size, input_shape=(x, y, z)\n",
    "#    MaxPooling2D, parameters: pool_size=pool_size\n",
    "#    Flatten,\n",
    "#    Dense, parameters: aantal outputs, activation='softmax'\n",
    "\n",
    "model = Sequential([Conv2D(num_filters, filter_size, input_shape=(28, 31, 1)),\n",
    "MaxPooling2D(pool_size= pool_size),\n",
    "Flatten(), Dense(10, activation='softmax')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "he8Zs-Sd2TID"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3240 - accuracy: 0.9072 - val_loss: 1.6977 - val_accuracy: 0.5373\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1477 - accuracy: 0.9587 - val_loss: 1.3506 - val_accuracy: 0.6344\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.1106 - accuracy: 0.9680 - val_loss: 1.2266 - val_accuracy: 0.6787\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x263f4e693a0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stap 3: het compilen van het model. \n",
    "# model.compile parameters: 'adam', loss='categorial_crossentropy', metrics=['accuracy']\n",
    "model.compile('adam', loss= keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Stap 4: fit het model. \n",
    "#    Data om op te trainen: train_images, to_categorial(train_labels)\n",
    "#    epochs = 3\n",
    "#    validation_data = test_images, to_categorial(test_labels)\n",
    "model.fit(train_images, to_categorical(train_labels), epochs = 3, validation_data = (test_images, to_categorical(test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 0s - loss: 1.2266 - accuracy: 0.6787\n",
      "0.6786999702453613\n"
     ]
    }
   ],
   "source": [
    "# Stap 5: evalueer het model\n",
    "test_loss, test_acc = model.evaluate(test_images,  to_categorical(test_labels), verbose=2)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pS8vERMeHh8j"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2980 - accuracy: 0.9131 - val_loss: 1.3336 - val_accuracy: 0.6404\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1139 - accuracy: 0.9662 - val_loss: 0.7537 - val_accuracy: 0.7800\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0853 - accuracy: 0.9745 - val_loss: 0.7182 - val_accuracy: 0.7988\n",
      "313/313 - 1s - loss: 0.7182 - accuracy: 0.7988\n",
      "0.798799991607666\n",
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0726 - accuracy: 0.9780 - val_loss: 0.5673 - val_accuracy: 0.8384\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0665 - accuracy: 0.9796 - val_loss: 0.5964 - val_accuracy: 0.8230\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0604 - accuracy: 0.9808 - val_loss: 0.5784 - val_accuracy: 0.8387\n",
      "313/313 - 1s - loss: 0.5784 - accuracy: 0.8387\n",
      "0.838699996471405\n"
     ]
    }
   ],
   "source": [
    "# Stap 6: extra layer(s). Wat gebeurt er als je een extra Conv Layer toevoegd aan je model? \n",
    "#    Voeg een extra layer(s) toe en train het model opnieuw.\n",
    "model2 = Sequential([Conv2D(num_filters, filter_size, input_shape=(28, 31, 1)),\n",
    "MaxPooling2D(pool_size= pool_size),\n",
    "Conv2D(num_filters, filter_size),\n",
    "Flatten(), Dense(10, activation='softmax')])\n",
    "\n",
    "model2.compile('adam', loss= keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\n",
    "model2.fit(train_images, to_categorical(train_labels), epochs = 3, validation_data = (test_images, to_categorical(test_labels)))\n",
    "test_loss, test_acc = model2.evaluate(test_images,  to_categorical(test_labels), verbose=2)\n",
    "print(test_acc)\n",
    "\n",
    "# Stap 7: parameters. Wat gebeurt er bijvoorbeeld als je geen softmax gebruikt maar een andere activatie? \n",
    "#    Pas op z'n minst 1 parameter aan en train je model opnieuw. \n",
    "model3 = Sequential([Conv2D(num_filters, filter_size, input_shape=(28, 31, 1)),\n",
    "MaxPooling2D(pool_size= pool_size),\n",
    "Conv2D(num_filters, filter_size),\n",
    "Flatten(), Dense(20, activation='relu')])\n",
    "\n",
    "model3.compile('adam', loss= keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\n",
    "model2.fit(train_images, to_categorical(train_labels), epochs = 3, validation_data = (test_images, to_categorical(test_labels)))\n",
    "test_loss, test_acc = model2.evaluate(test_images,  to_categorical(test_labels), verbose=2)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 31\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data inladen\n",
    "(train_images10, train_labels10), (test_images10, test_labels10) = cifar10.load_data()\n",
    "print(len(train_images[0]), len(train_images[0][0]))\n",
    "\n",
    "plt.imshow(train_images10[50])\n",
    "plt.show()\n",
    "\n",
    "# Normalizeren\n",
    "train_images10, test_images10 = train_images10 / 255.0, test_images10 / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stap 8: bouw je eigen CNN voor de CIFAR-10 dataset. \n",
    "# Tip: gebruik meerdere Conv2D en MaxPooling layers\n",
    "# LET OP: gebruik 'softmax' alleen bij je laatste Dense layer. Gebruik 'relu' voor de andere Conv2D/Dense layers. \n",
    "model_cif = Sequential([Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "Conv2D(32, (2, 2), activation='relu'),\n",
    "MaxPooling2D((2, 2)),\n",
    "Conv2D(64, (2, 2), activation='relu'),\n",
    "Conv2D(64, (2, 2), activation='relu'),\n",
    "MaxPooling2D((2, 2)),\n",
    "Conv2D(128, (2, 2), activation='relu'),\n",
    "Conv2D(128, (2, 2), activation='relu'),\n",
    "MaxPooling2D((2, 2)),\n",
    "Flatten(),\n",
    "Dense(10, activation='softmax')])\n",
    "\n",
    "model_cif.compile(\n",
    "    'adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1563/1563 [==============================] - 45s 29ms/step - loss: 1.4382 - accuracy: 0.4808 - val_loss: 1.2321 - val_accuracy: 0.5568\n",
      "Epoch 2/20\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 1.1202 - accuracy: 0.6055 - val_loss: 1.0432 - val_accuracy: 0.6287\n",
      "Epoch 3/20\n",
      "1563/1563 [==============================] - 48s 31ms/step - loss: 0.9490 - accuracy: 0.6659 - val_loss: 0.9492 - val_accuracy: 0.6681\n",
      "Epoch 4/20\n",
      "1563/1563 [==============================] - 44s 28ms/step - loss: 0.8295 - accuracy: 0.7097 - val_loss: 0.9081 - val_accuracy: 0.6900\n",
      "Epoch 5/20\n",
      "1563/1563 [==============================] - 44s 28ms/step - loss: 0.7384 - accuracy: 0.7424 - val_loss: 0.8171 - val_accuracy: 0.7160\n",
      "Epoch 6/20\n",
      "1563/1563 [==============================] - 45s 29ms/step - loss: 0.6740 - accuracy: 0.7646 - val_loss: 0.7994 - val_accuracy: 0.7270\n",
      "Epoch 7/20\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 0.6080 - accuracy: 0.7878 - val_loss: 0.8182 - val_accuracy: 0.7197\n",
      "Epoch 8/20\n",
      "1563/1563 [==============================] - 45s 29ms/step - loss: 0.5573 - accuracy: 0.8062 - val_loss: 0.7741 - val_accuracy: 0.7427\n",
      "Epoch 9/20\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 0.5085 - accuracy: 0.8208 - val_loss: 0.9077 - val_accuracy: 0.7142\n",
      "Epoch 10/20\n",
      "1563/1563 [==============================] - 43s 28ms/step - loss: 0.4655 - accuracy: 0.8357 - val_loss: 0.8497 - val_accuracy: 0.7288\n",
      "Epoch 11/20\n",
      "1563/1563 [==============================] - 44s 28ms/step - loss: 0.4221 - accuracy: 0.8513 - val_loss: 0.8477 - val_accuracy: 0.7352\n",
      "Epoch 12/20\n",
      "1563/1563 [==============================] - 45s 29ms/step - loss: 0.3866 - accuracy: 0.8628 - val_loss: 0.9137 - val_accuracy: 0.7256\n",
      "Epoch 13/20\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 0.3595 - accuracy: 0.8731 - val_loss: 0.9054 - val_accuracy: 0.7357\n",
      "Epoch 14/20\n",
      "1563/1563 [==============================] - 44s 28ms/step - loss: 0.3314 - accuracy: 0.8815 - val_loss: 1.0073 - val_accuracy: 0.7261\n",
      "Epoch 15/20\n",
      "1563/1563 [==============================] - 44s 28ms/step - loss: 0.3048 - accuracy: 0.8912 - val_loss: 1.0554 - val_accuracy: 0.7265\n",
      "Epoch 16/20\n",
      "1563/1563 [==============================] - 44s 28ms/step - loss: 0.2877 - accuracy: 0.8991 - val_loss: 1.0736 - val_accuracy: 0.7319\n",
      "Epoch 17/20\n",
      "1563/1563 [==============================] - 45s 28ms/step - loss: 0.2658 - accuracy: 0.9046 - val_loss: 1.1146 - val_accuracy: 0.7260\n",
      "Epoch 18/20\n",
      "1563/1563 [==============================] - 44s 28ms/step - loss: 0.2392 - accuracy: 0.9133 - val_loss: 1.2086 - val_accuracy: 0.7281\n",
      "Epoch 19/20\n",
      "1563/1563 [==============================] - 42s 27ms/step - loss: 0.2334 - accuracy: 0.9157 - val_loss: 1.2349 - val_accuracy: 0.7310\n",
      "Epoch 20/20\n",
      "1563/1563 [==============================] - 43s 27ms/step - loss: 0.2178 - accuracy: 0.9217 - val_loss: 1.2631 - val_accuracy: 0.7327\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x263fa1561c0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cif.fit(\n",
    "  train_images10,\n",
    "  to_categorical(train_labels10),\n",
    "  epochs= 20,\n",
    "  validation_data=(test_images10, to_categorical(test_labels10)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 3s - loss: 1.2631 - accuracy: 0.7327\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model_cif.evaluate(test_images10,  to_categorical(test_labels10), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.732699990272522\n"
     ]
    }
   ],
   "source": [
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bronnen\n",
    "* https://victorzhou.com/blog/keras-cnn-tutorial/ Bezocht: 9/3/2020\n",
    "* https://www.tensorflow.org/tutorials/images/cnn Bezocht: 13/3/2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Workshop_CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
